{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4f4JG1gdKqj"
   },
   "source": [
    "# AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOBJ8UCXdY0g"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LvGeU1CeCtg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pM04FyMudkoK"
   },
   "source": [
    "## Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJw2p3-Cewo4"
   },
   "outputs": [],
   "source": [
    "# We won't be using this dataset.\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTIbE2tkdkwP"
   },
   "source": [
    "## Preparing the training set and the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2usLKJBEgPE2"
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter='\\t')\n",
    "training_set = np.array(training_set, dtype='int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter='\\t')\n",
    "test_set = np.array(test_set, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCf8HjSydk4s"
   },
   "source": [
    "## Getting the number of users and movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPaGZqdniC5m"
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-w4-hVidlAm"
   },
   "source": [
    "## Converting the data into an array with users in lines and movies in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wASs2YFiDaa"
   },
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(nb_users):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users + 1]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users + 1]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMmhuUpldlHo"
   },
   "source": [
    "## Converting the data into Torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwD-KD8yiEEw"
   },
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kkL8NkkdlZj"
   },
   "source": [
    "## Creating the architecture of the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU2nyh76iE6M"
   },
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gy59alAdloL"
   },
   "source": [
    "## Training the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FEz9hRaciFTs",
    "outputId": "0f6ed0d0-09c4-46c0-bfe6-70031d76b491",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: tensor(1.7652)\n",
      "epoch: 2, loss: tensor(1.0965)\n",
      "epoch: 3, loss: tensor(1.0532)\n",
      "epoch: 4, loss: tensor(1.0385)\n",
      "epoch: 5, loss: tensor(1.0312)\n",
      "epoch: 6, loss: tensor(1.0265)\n",
      "epoch: 7, loss: tensor(1.0240)\n",
      "epoch: 8, loss: tensor(1.0221)\n",
      "epoch: 9, loss: tensor(1.0208)\n",
      "epoch: 10, loss: tensor(1.0197)\n",
      "epoch: 11, loss: tensor(1.0189)\n",
      "epoch: 12, loss: tensor(1.0184)\n",
      "epoch: 13, loss: tensor(1.0180)\n",
      "epoch: 14, loss: tensor(1.0175)\n",
      "epoch: 15, loss: tensor(1.0171)\n",
      "epoch: 16, loss: tensor(1.0169)\n",
      "epoch: 17, loss: tensor(1.0166)\n",
      "epoch: 18, loss: tensor(1.0166)\n",
      "epoch: 19, loss: tensor(1.0163)\n",
      "epoch: 20, loss: tensor(1.0162)\n",
      "epoch: 21, loss: tensor(1.0159)\n",
      "epoch: 22, loss: tensor(1.0159)\n",
      "epoch: 23, loss: tensor(1.0160)\n",
      "epoch: 24, loss: tensor(1.0159)\n",
      "epoch: 25, loss: tensor(1.0159)\n",
      "epoch: 26, loss: tensor(1.0154)\n",
      "epoch: 27, loss: tensor(1.0154)\n",
      "epoch: 28, loss: tensor(1.0152)\n",
      "epoch: 29, loss: tensor(1.0130)\n",
      "epoch: 30, loss: tensor(1.0115)\n",
      "epoch: 31, loss: tensor(1.0100)\n",
      "epoch: 32, loss: tensor(1.0078)\n",
      "epoch: 33, loss: tensor(1.0076)\n",
      "epoch: 34, loss: tensor(1.0042)\n",
      "epoch: 35, loss: tensor(1.0032)\n",
      "epoch: 36, loss: tensor(0.9996)\n",
      "epoch: 37, loss: tensor(0.9996)\n",
      "epoch: 38, loss: tensor(0.9964)\n",
      "epoch: 39, loss: tensor(0.9932)\n",
      "epoch: 40, loss: tensor(0.9926)\n",
      "epoch: 41, loss: tensor(0.9935)\n",
      "epoch: 42, loss: tensor(0.9924)\n",
      "epoch: 43, loss: tensor(0.9899)\n",
      "epoch: 44, loss: tensor(0.9880)\n",
      "epoch: 45, loss: tensor(0.9855)\n",
      "epoch: 46, loss: tensor(0.9810)\n",
      "epoch: 47, loss: tensor(0.9852)\n",
      "epoch: 48, loss: tensor(0.9819)\n",
      "epoch: 49, loss: tensor(0.9807)\n",
      "epoch: 50, loss: tensor(0.9799)\n",
      "epoch: 51, loss: tensor(0.9789)\n",
      "epoch: 52, loss: tensor(0.9769)\n",
      "epoch: 53, loss: tensor(0.9740)\n",
      "epoch: 54, loss: tensor(0.9729)\n",
      "epoch: 55, loss: tensor(0.9725)\n",
      "epoch: 56, loss: tensor(0.9766)\n",
      "epoch: 57, loss: tensor(0.9725)\n",
      "epoch: 58, loss: tensor(0.9708)\n",
      "epoch: 59, loss: tensor(0.9722)\n",
      "epoch: 60, loss: tensor(0.9719)\n",
      "epoch: 61, loss: tensor(0.9697)\n",
      "epoch: 62, loss: tensor(0.9628)\n",
      "epoch: 63, loss: tensor(0.9622)\n",
      "epoch: 64, loss: tensor(0.9593)\n",
      "epoch: 65, loss: tensor(0.9581)\n",
      "epoch: 66, loss: tensor(0.9575)\n",
      "epoch: 67, loss: tensor(0.9582)\n",
      "epoch: 68, loss: tensor(0.9641)\n",
      "epoch: 69, loss: tensor(0.9634)\n",
      "epoch: 70, loss: tensor(0.9571)\n",
      "epoch: 71, loss: tensor(0.9548)\n",
      "epoch: 72, loss: tensor(0.9518)\n",
      "epoch: 73, loss: tensor(0.9518)\n",
      "epoch: 74, loss: tensor(0.9499)\n",
      "epoch: 75, loss: tensor(0.9496)\n",
      "epoch: 76, loss: tensor(0.9499)\n",
      "epoch: 77, loss: tensor(0.9513)\n",
      "epoch: 78, loss: tensor(0.9476)\n",
      "epoch: 79, loss: tensor(0.9467)\n",
      "epoch: 80, loss: tensor(0.9448)\n",
      "epoch: 81, loss: tensor(0.9471)\n",
      "epoch: 82, loss: tensor(0.9458)\n",
      "epoch: 83, loss: tensor(0.9433)\n",
      "epoch: 84, loss: tensor(0.9416)\n",
      "epoch: 85, loss: tensor(0.9413)\n",
      "epoch: 86, loss: tensor(0.9441)\n",
      "epoch: 87, loss: tensor(0.9422)\n",
      "epoch: 88, loss: tensor(0.9414)\n",
      "epoch: 89, loss: tensor(0.9409)\n",
      "epoch: 90, loss: tensor(0.9397)\n",
      "epoch: 91, loss: tensor(0.9389)\n",
      "epoch: 92, loss: tensor(0.9382)\n",
      "epoch: 93, loss: tensor(0.9377)\n",
      "epoch: 94, loss: tensor(0.9368)\n",
      "epoch: 95, loss: tensor(0.9372)\n",
      "epoch: 96, loss: tensor(0.9364)\n",
      "epoch: 97, loss: tensor(0.9362)\n",
      "epoch: 98, loss: tensor(0.9351)\n",
      "epoch: 99, loss: tensor(0.9350)\n",
      "epoch: 100, loss: tensor(0.9339)\n",
      "epoch: 101, loss: tensor(0.9345)\n",
      "epoch: 102, loss: tensor(0.9332)\n",
      "epoch: 103, loss: tensor(0.9336)\n",
      "epoch: 104, loss: tensor(0.9321)\n",
      "epoch: 105, loss: tensor(0.9328)\n",
      "epoch: 106, loss: tensor(0.9310)\n",
      "epoch: 107, loss: tensor(0.9315)\n",
      "epoch: 108, loss: tensor(0.9304)\n",
      "epoch: 109, loss: tensor(0.9313)\n",
      "epoch: 110, loss: tensor(0.9297)\n",
      "epoch: 111, loss: tensor(0.9300)\n",
      "epoch: 112, loss: tensor(0.9291)\n",
      "epoch: 113, loss: tensor(0.9294)\n",
      "epoch: 114, loss: tensor(0.9285)\n",
      "epoch: 115, loss: tensor(0.9286)\n",
      "epoch: 116, loss: tensor(0.9276)\n",
      "epoch: 117, loss: tensor(0.9276)\n",
      "epoch: 118, loss: tensor(0.9274)\n",
      "epoch: 119, loss: tensor(0.9279)\n",
      "epoch: 120, loss: tensor(0.9265)\n",
      "epoch: 121, loss: tensor(0.9273)\n",
      "epoch: 122, loss: tensor(0.9257)\n",
      "epoch: 123, loss: tensor(0.9259)\n",
      "epoch: 124, loss: tensor(0.9247)\n",
      "epoch: 125, loss: tensor(0.9262)\n",
      "epoch: 126, loss: tensor(0.9252)\n",
      "epoch: 127, loss: tensor(0.9258)\n",
      "epoch: 128, loss: tensor(0.9243)\n",
      "epoch: 129, loss: tensor(0.9247)\n",
      "epoch: 130, loss: tensor(0.9236)\n",
      "epoch: 131, loss: tensor(0.9239)\n",
      "epoch: 132, loss: tensor(0.9230)\n",
      "epoch: 133, loss: tensor(0.9241)\n",
      "epoch: 134, loss: tensor(0.9228)\n",
      "epoch: 135, loss: tensor(0.9234)\n",
      "epoch: 136, loss: tensor(0.9224)\n",
      "epoch: 137, loss: tensor(0.9229)\n",
      "epoch: 138, loss: tensor(0.9221)\n",
      "epoch: 139, loss: tensor(0.9224)\n",
      "epoch: 140, loss: tensor(0.9212)\n",
      "epoch: 141, loss: tensor(0.9216)\n",
      "epoch: 142, loss: tensor(0.9204)\n",
      "epoch: 143, loss: tensor(0.9213)\n",
      "epoch: 144, loss: tensor(0.9202)\n",
      "epoch: 145, loss: tensor(0.9206)\n",
      "epoch: 146, loss: tensor(0.9200)\n",
      "epoch: 147, loss: tensor(0.9204)\n",
      "epoch: 148, loss: tensor(0.9198)\n",
      "epoch: 149, loss: tensor(0.9199)\n",
      "epoch: 150, loss: tensor(0.9192)\n",
      "epoch: 151, loss: tensor(0.9196)\n",
      "epoch: 152, loss: tensor(0.9185)\n",
      "epoch: 153, loss: tensor(0.9191)\n",
      "epoch: 154, loss: tensor(0.9186)\n",
      "epoch: 155, loss: tensor(0.9192)\n",
      "epoch: 156, loss: tensor(0.9179)\n",
      "epoch: 157, loss: tensor(0.9184)\n",
      "epoch: 158, loss: tensor(0.9182)\n",
      "epoch: 159, loss: tensor(0.9184)\n",
      "epoch: 160, loss: tensor(0.9176)\n",
      "epoch: 161, loss: tensor(0.9179)\n",
      "epoch: 162, loss: tensor(0.9173)\n",
      "epoch: 163, loss: tensor(0.9174)\n",
      "epoch: 164, loss: tensor(0.9163)\n",
      "epoch: 165, loss: tensor(0.9172)\n",
      "epoch: 166, loss: tensor(0.9170)\n",
      "epoch: 167, loss: tensor(0.9171)\n",
      "epoch: 168, loss: tensor(0.9165)\n",
      "epoch: 169, loss: tensor(0.9164)\n",
      "epoch: 170, loss: tensor(0.9158)\n",
      "epoch: 171, loss: tensor(0.9161)\n",
      "epoch: 172, loss: tensor(0.9156)\n",
      "epoch: 173, loss: tensor(0.9159)\n",
      "epoch: 174, loss: tensor(0.9153)\n",
      "epoch: 175, loss: tensor(0.9157)\n",
      "epoch: 176, loss: tensor(0.9149)\n",
      "epoch: 177, loss: tensor(0.9153)\n",
      "epoch: 178, loss: tensor(0.9151)\n",
      "epoch: 179, loss: tensor(0.9155)\n",
      "epoch: 180, loss: tensor(0.9145)\n",
      "epoch: 181, loss: tensor(0.9147)\n",
      "epoch: 182, loss: tensor(0.9141)\n",
      "epoch: 183, loss: tensor(0.9143)\n",
      "epoch: 184, loss: tensor(0.9142)\n",
      "epoch: 185, loss: tensor(0.9140)\n",
      "epoch: 186, loss: tensor(0.9140)\n",
      "epoch: 187, loss: tensor(0.9138)\n",
      "epoch: 188, loss: tensor(0.9135)\n",
      "epoch: 189, loss: tensor(0.9138)\n",
      "epoch: 190, loss: tensor(0.9138)\n",
      "epoch: 191, loss: tensor(0.9136)\n",
      "epoch: 192, loss: tensor(0.9133)\n",
      "epoch: 193, loss: tensor(0.9129)\n",
      "epoch: 194, loss: tensor(0.9130)\n",
      "epoch: 195, loss: tensor(0.9129)\n",
      "epoch: 196, loss: tensor(0.9128)\n",
      "epoch: 197, loss: tensor(0.9127)\n",
      "epoch: 198, loss: tensor(0.9128)\n",
      "epoch: 199, loss: tensor(0.9122)\n",
      "epoch: 200, loss: tensor(0.9122)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(nb_epoch):\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae.forward(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data * mean_corrector)\n",
    "            s += 1.0\n",
    "            optimizer.step()\n",
    "    print(\"epoch: \" + str(epoch + 1) + \", loss: \" + str(train_loss / s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bak5uc8gd-gX"
   },
   "source": [
    "## Testing the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5ztvzYRtiGCz",
    "outputId": "d0e8ea8b-9ac4-40e5-a19a-7fcfc6934d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.9520)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.0\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae.forward(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data * mean_corrector)\n",
    "        s += 1.0\n",
    "print(\"test loss: \" + str(test_loss / s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
